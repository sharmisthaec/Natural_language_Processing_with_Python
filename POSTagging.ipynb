{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Goal of a POS tagger is to assign linguistic (mostly grammatical) information to sub-sentential units. Such units are called tokens and, most of the time, correspond to words and symbols (e.g. punctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), (',', ','), ('I', 'PRP'), ('Dr.', 'NNP'), ('Chetana', 'NNP'), ('.', '.')]\n",
      "[('Welcome', 'JJ'), ('lab', 'NN'), ('session', 'NN'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('.', '.')]\n",
      "[('NLP', 'NNP'), ('interesting', 'JJ'), ('area', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "statement = \"Hello all, I am Dr. Chetana. \" \\\n",
    "            \"Welcome to the lab session of Natural Language Processing(NLP). \" \\\n",
    "            \"NLP is a very interesting area.\"\n",
    "\n",
    "# sent_tokenize is one of instances of  \n",
    "# PunktSentenceTokenizer from the nltk.tokenize.punkt module \n",
    "  \n",
    "tokenized = sent_tokenize(statement) \n",
    "for i in tokenized: \n",
    "      \n",
    "    # Word tokenizers is used to find the words  \n",
    "    # and punctuation in a string \n",
    "    wordsList = nltk.word_tokenize(i) \n",
    "  \n",
    "    # removing stop words from wordList \n",
    "    wordsList = [w for w in wordsList if not w in stop_words]  \n",
    "  \n",
    "    #  Using a Tagger. Which is part-of-speech  \n",
    "    # tagger or POS-tagger.  \n",
    "    tagged = nltk.pos_tag(wordsList) \n",
    "  \n",
    "    print(tagged) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK includes a diverse set of corpora which can be read using the nltk.corpus package. \n",
    "# Corpus : Body of text, singular. Corpora is the plural of this.\n",
    "# Most corpora consist of a set of files, each containing a document (or other pieces of text). \n",
    "# Each corpus reader provides a variety of methods to read data from the corpus, \n",
    "# depending on the format of the corpus. \n",
    "# NLTK's data package also contains a wide variety of annotated corpora. \n",
    "# For example, the Brown Corpus is annotated with part-of-speech tags\n",
    "# Indian Language POS-Tagged Corpus includes samples of Indian text annotated with part-of-speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Others', 'NNS'), (',', ','), ('which', 'WDT'), ('are', 'BER'), ('reached', 'VBN'), ('by', 'IN'), ('walking', 'VBG'), ('up', 'RP'), ('a', 'AT'), ('single', 'AP'), ('flight', 'NN'), ('of', 'IN'), ('stairs', 'NNS'), (',', ','), ('have', 'HV'), ('balconies', 'NNS'), ('.', '.')]\n",
      "0.9349006503968017\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Unigram tag using brown corpus.\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')\n",
    "# We train a UnigramTagger by specifying tagged sentence data as a parameter\n",
    "# when we initialize the tagger.\n",
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "print(unigram_tagger.tag(brown_sents[2008]))\n",
    "print(unigram_tagger.evaluate(brown_tagged_sents))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# POS tagging is traditionally a supervised learning question, we need some sentences with POS tags to train and test with.\n",
    "In practice, people label a bunch of sentences then split them to make a test and train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8630364649525858\n"
     ]
    }
   ],
   "source": [
    "from nltk import UnigramTagger\n",
    "from nltk.corpus import brown\n",
    "# Use the brown corpus with universal tagset for readability\n",
    "tagged_sentences = brown.tagged_sents(categories=\"news\", tagset=\"universal\")\n",
    "\n",
    "# 20% of the data for testing, and 80% for training\n",
    "i = int(len(tagged_sentences)*0.2)\n",
    "train_sentences = tagged_sentences[i:]\n",
    "test_sentences = tagged_sentences[:i]\n",
    "\n",
    "# Train the tagger with train sentences\n",
    "unigram_tagger = UnigramTagger(train_sentences)\n",
    "# Evaluate with test sentences\n",
    "# default evaluation metric for nltk taggers is accuracy\n",
    "accuracy = unigram_tagger.evaluate(test_sentences)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           .       1.00      1.00      1.00      2107\n",
      "         ADJ       0.89      0.79      0.84      1341\n",
      "         ADP       0.97      0.92      0.94      2621\n",
      "         ADV       0.93      0.79      0.86       573\n",
      "        CONJ       1.00      1.00      1.00       453\n",
      "         DET       1.00      0.99      1.00      2456\n",
      "        NOUN       0.96      0.76      0.85      6265\n",
      "         NUM       0.99      0.85      0.92       379\n",
      "        None       0.00      0.00      0.00         0\n",
      "        PRON       1.00      0.96      0.98       502\n",
      "         PRT       0.69      0.96      0.80       481\n",
      "        VERB       0.96      0.83      0.89      3274\n",
      "           X       0.10      0.17      0.12         6\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     20458\n",
      "   macro avg       0.81      0.77      0.78     20458\n",
      "weighted avg       0.96      0.86      0.91     20458\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Other measures to evaluate the quality of POS tagging\n",
    "tagged_test_sentences = unigram_tagger.tag_sents([[token for token,tag in sent] for sent in test_sentences])\n",
    "gold = [str(tag) for sentence in test_sentences for token,tag in sentence]\n",
    "pred = [str(tag) for sentence in tagged_test_sentences for token,tag in sentence]\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(gold, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
